{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "# from dotenv import load_dotenv\n",
    "# import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read QA questions from the json file\n",
    "# with open('shuffled_QA_pairs_econ_part1.json') as f:\n",
    "#     data1 = json.load(f)\n",
    "\n",
    "# with open('shuffled_QA_pairs_econ_part2.json') as f:\n",
    "#     data2 = json.load(f)\n",
    "\n",
    "# data_merged = data1 + data2\n",
    "# question_set = set()\n",
    "# data = []\n",
    "# for row in data_merged:\n",
    "#     if row['question'] not in question_set:\n",
    "#         question_set.add(row['question'])\n",
    "#         data.append(row)\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "# with open('shuffled_QA_pairs_econ.json', 'w') as f:\n",
    "#     json.dump(data, f)\n",
    "\n",
    "with open('shuffled_QA_pairs_econ.json') as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))\n",
    "# Load the .env file\n",
    "# load_dotenv()\n",
    "# client = openai.OpenAI()\n",
    "# claude_client = anthropic.Anthropic()\n",
    "client = openai.AzureOpenAI(api_version='2024-06-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "log = defaultdict(dict)\n",
    "\n",
    "\n",
    "for generated_qa in tqdm(data, desc=\"Generating answers\"):\n",
    "    question = generated_qa['question']\n",
    "    correct_answer = generated_qa['correct_answer']\n",
    "    choices = generated_qa['mcq']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Question: {choices}\n",
    "    Which of the above choices best answers the question? Give a reason for your choice first, then give your choice. Think step by step.\n",
    "    Reply a JSON object with the following format:\n",
    "    {{\n",
    "        \"reason\": \"string\",\n",
    "        \"choice\": \"char\",\n",
    "    }}\n",
    "    \"\"\"\n",
    "    temperature = 0.0\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a student answering a textbook problem.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        answer_json = json.loads(response.choices[0].message.content)\n",
    "        log[question]['reason'] = answer_json['reason']\n",
    "        log[question]['gpt_choice'] = answer_json['choice']\n",
    "        log[question]['correct_choice'] = correct_answer + 1\n",
    "    except:\n",
    "        print(response.choices[0].message.content)\n",
    "        log[question]['reason'] = \"\"\n",
    "        log[question]['gpt_choice'] = \"\"\n",
    "        log[question]['correct_choice'] = correct_answer + 1\n",
    "\n",
    "    char2num_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4}\n",
    "    log[question]['incorrect_choices'] = [char2num_map[char.upper()] for char in log[question]['incorrect_choices']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "total_count = len(log)\n",
    "\n",
    "for _, answers in log.items():\n",
    "    if answers['gpt_choice'] == answers['correct_choice']:\n",
    "        correct_count += 1\n",
    "\n",
    "percentage_correct = (correct_count / total_count) * 100\n",
    "print(f\"Percentage of correct choices: {percentage_correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('log_all_cot.pkl', 'wb') as f:\n",
    "    pickle.dump(log, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select all wrong answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shuffled_QA_pairs_econ.json') as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))\n",
    "client = openai.AzureOpenAI(api_version='2024-06-01')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "log = defaultdict(dict)\n",
    "\n",
    "\n",
    "for generated_qa in tqdm(data, desc=\"Generating answers\"):\n",
    "    question = generated_qa['question']\n",
    "    correct_answer = generated_qa['correct_answer']\n",
    "    choices = generated_qa['mcq']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Question: {choices}\n",
    "    Select all wrong choices. Give reasons for your choices, then give your choices. Think step by step.\n",
    "    Reply a JSON object with the following format:\n",
    "    {{\n",
    "        \"analysis\": {{\n",
    "            \"choice_A\": \"Reason why choice A is correct/incorrect\",\n",
    "            \"choice_B\": \"Reason why choice B is correct/incorrect\",\n",
    "            \"...\": \"...\"\n",
    "        }},\n",
    "        \"incorrect_choices\": [char, char, ...]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    print(prompt)\n",
    "    temperature = 0.0\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a student answering a textbook problem.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        answer_json = json.loads(response.choices[0].message.content)\n",
    "        log[question]['analysis'] = answer_json['analysis']\n",
    "        log[question]['incorrect_choices'] = answer_json['incorrect_choices']\n",
    "        log[question]['correct_choice'] = correct_answer + 1\n",
    "    except:\n",
    "        print(response.choices[0].message.content)\n",
    "        log[question]['analysis'] = \"\"\n",
    "        log[question]['incorrect_choices'] = []\n",
    "        log[question]['correct_choice'] = correct_answer + 1\n",
    "\n",
    "    char2num_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4}\n",
    "    log[question]['incorrect_choices'] = [char2num_map[char.upper()] for char in log[question]['incorrect_choices']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('log_pick_wrong.pkl', 'wb') as f:\n",
    "    pickle.dump(log, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select all correct answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 338/338 [28:15<00:00,  5.02s/it] \n"
     ]
    }
   ],
   "source": [
    "with open('shuffled_QA_pairs_econ.json') as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))\n",
    "client = openai.AzureOpenAI(api_version='2024-06-01')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "log = defaultdict(dict)\n",
    "\n",
    "\n",
    "for generated_qa in tqdm(data, desc=\"Generating answers\"):\n",
    "    question = generated_qa['question']\n",
    "    correct_answer = generated_qa['correct_answer']\n",
    "    choices = generated_qa['mcq']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Question: {choices}\n",
    "    Select all correct choices. Give reasons for your choices, then give your choices. Think step by step.\n",
    "    Reply a JSON object with the following format:\n",
    "    {{\n",
    "        \"analysis\": {{\n",
    "            \"choice_A\": \"Reason why choice A is correct/incorrect\",\n",
    "            \"choice_B\": \"Reason why choice B is correct/incorrect\",\n",
    "            \"...\": \"...\"\n",
    "        }},\n",
    "        \"correct_choices\": [char, char, ...]\n",
    "    }}\n",
    "    \"\"\"\n",
    "    temperature = 0.0\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a student answering a textbook problem.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        answer_json = json.loads(response.choices[0].message.content)\n",
    "        log[question]['analysis'] = answer_json['analysis']\n",
    "        log[question]['correct_choices'] = answer_json['correct_choices']\n",
    "        log[question]['correct_choice'] = correct_answer + 1\n",
    "    except:\n",
    "        print(response.choices[0].message.content)\n",
    "        log[question]['analysis'] = \"\"\n",
    "        log[question]['correct_choices'] = []\n",
    "        log[question]['correct_choice'] = correct_answer + 1\n",
    "\n",
    "    char2num_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4}\n",
    "    log[question]['correct_choices'] = [char2num_map[char.upper()] for char in log[question]['correct_choices']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('log_pick_correct.pkl', 'rb') as f:\n",
    "    log = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[list(log.keys())[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read QA questions from the json file\n",
    "with open('shuffled_QA_pairs_econ.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Load the .env file\n",
    "# load_dotenv()\n",
    "# client = openai.OpenAI()\n",
    "# claude_client = anthropic.Anthropic()\n",
    "client = openai.AzureOpenAI(api_version='2024-06-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, q in enumerate(data):    \n",
    "    # remove the empty newlines and first few numbers followed by a period in the question by regex    \n",
    "    clean_question = re.sub(r'^\\s*\\d+\\.\\s*', '', q['question'])    \n",
    "    data[i]['clean_question'] = clean_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "classification_log = defaultdict(dict)\n",
    "\n",
    "for generated_qa in tqdm(data, desc=\"Generating answers\"):\n",
    "    question_key = generated_qa['question']\n",
    "    question = generated_qa['clean_question']\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    You will be presented with a problem pulled from a finance textbook. \n",
    "    Your goal is to determine three things: whether the question has enough information to be answered, if it does, what type of question it is, and if it is fit for use as a multiple-choice question.\n",
    "    \n",
    "    When determining if the question has enough information, classify it as one of the following:\n",
    "    1. Incomplete: Is the question complete? Are there any missing words or phrases, or references to other parts of the text that are not included?\n",
    "    2. Missing reference: Does the question contain all the reference materials? Are there any missing tables, charts, or other information that is necessary to answer the question?\n",
    "    3. Missing context: Does the question contain all the necessary context? Does it ask for information from sections in the textbook, which are not included in the question?\n",
    "    9. Other: The question does not have enough information, but it does not fit into any of the above categories.\n",
    "    0. Complete: The question has all the necessary information to be answered.\n",
    "\n",
    "    When determining the type of question, classify the question as one of the following:\n",
    "    1. Conceptual: The question asks for an explanation of a concept or theory.\n",
    "    2. Computation: The question asks for a calculation or numerical answer.\n",
    "    3. True/False: The question asks for a true or false answer.\n",
    "    4. Graphical: The question asks for a graph or chart to be drawn.\n",
    "    5. Reasoning: The question asks for a logical explanation or reasoning.\n",
    "    9. Other: The question does not fit into any of the above categories.\n",
    "\n",
    "    When determining if the question is fit for use as a multiple-choice question, simply answer yes or no by assigning a 1 or 0, respectively.\n",
    "\n",
    "    You will also be asked to provide a reason for your answer. Please provide a detailed explanation for your choice.\n",
    "    When you answer, use the associated number to indicate your choice for each of the three questions.\n",
    "    For example, a complete question that is a conceptual question and is not fit for use as a multiple-choice question would be answered with:\n",
    "    0, 1, 0\n",
    "    \"\"\"\n",
    "\n",
    "    question_prompt = f\"\"\"\n",
    "    Question: {question}\n",
    "    For your answer, reply a JSON object with the following format:\n",
    "    {{\n",
    "        \"completeness_reason\": \"string\",\n",
    "        \"type_reason\": \"string\",\n",
    "        \"mcq_reason\": \"string\",\n",
    "        \"completeness_choice\": \"int\",\n",
    "        \"type_choice\": \"int\",\n",
    "        \"mcq_choice\": \"int\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    temperature = 0.0\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": question_prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "    \n",
    "    answer_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    answer_dict = {\n",
    "        \"completeness_reason\": answer_json['completeness_reason'],\n",
    "        \"type_reason\": answer_json['type_reason'],\n",
    "        \"mcq_reason\": answer_json['mcq_reason'],\n",
    "        \"completeness_choice\": answer_json['completeness_choice'],\n",
    "        \"type_choice\": answer_json['type_choice'],\n",
    "        \"mcq_choice\": answer_json['mcq_choice']\n",
    "    }\n",
    "\n",
    "    classification_log[question_key] = answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('classification_log_all.pkl', 'wb') as f:\n",
    "    pickle.dump(classification_log, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('log.pkl', 'rb') as f:\n",
    "    log = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('classification_log.pkl', 'rb') as f:\n",
    "    classification_log = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_log[list(classification_log.keys())[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for original_qa in data:\n",
    "    question = original_qa['question']\n",
    "    log[question]['question'] = original_qa['mcq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, individual_data in enumerate(data):\n",
    "    question = individual_data['question']\n",
    "    log[question]['index'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for _ in range(10):\n",
    "    question = random.choice(data)['question']\n",
    "    print(f\"Question: {log[question]['question']}\")\n",
    "    print(f\"Correct choice: {log[question]['correct_choice'] + 1}\")\n",
    "    print(f\"GPT choice: {log[question]['gpt_choice']}\")\n",
    "    print(f\"Reason: {log[question]['reason']}\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic.types.beta.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.beta.messages.batch_create_params import Request\n",
    "requests = []\n",
    "\n",
    "for generated_qa in tqdm(data, desc=\"Generating answers\"):\n",
    "    question = generated_qa['question']\n",
    "    correct_answer = generated_qa['correct_answer']\n",
    "    choices = generated_qa['mcq']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Question: {choices}\n",
    "    Which of the above choices best answers the question? Give a reason for your choice first, then give your choice as an integer between 0 and 3.\n",
    "    Reply only a JSON object with the following format:\n",
    "    {{\n",
    "        \"reason\": \"string\",\n",
    "        \"choice\": \"int\",\n",
    "    }}\n",
    "    \"\"\"\n",
    "    temperature = 0.0\n",
    "    params = MessageCreateParamsNonStreaming(\n",
    "                model=\"claude-3-5-haiku-latest\",\n",
    "                system = \"You are a student answering a textbook problem.\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=1000,\n",
    "            )\n",
    "    \n",
    "    custom_id = str(hash(question))\n",
    "    requests.append(Request(custom_id=custom_id, params=params))\n",
    "    break\n",
    "    \n",
    "message_batch = claude_client.beta.messages.batches.create(requests=requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_log = defaultdict(dict)\n",
    "\n",
    "batch_id = message_batch.id\n",
    "\n",
    "for result in claude_client.beta.messages.batches.results(batch_id):\n",
    "    if result.result.type == \"succeeded\":\n",
    "        print(f\"Success! {result.custom_id}\")\n",
    "\n",
    "\n",
    "# answer_json = json.loads(response.content[0].text)\n",
    "\n",
    "#     anthropic_log[question]['reason'] = answer_json['reason']\n",
    "#     anthropic_log[question]['gpt_choice'] = answer_json['choice']\n",
    "#     anthropic_log[question]['correct_choice'] = correct_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "total_count = len(anthropic_log)\n",
    "\n",
    "for _, answers in log.items():\n",
    "    if answers['gpt_choice'] == answers['correct_choice'] + 1:\n",
    "        correct_count += 1\n",
    "\n",
    "percentage_correct = (correct_count / total_count) * 100\n",
    "print(f\"Percentage of correct choices: {percentage_correct:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
