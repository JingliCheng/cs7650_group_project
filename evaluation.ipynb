{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read QA questions from the json file\n",
    "with open('shuffled_QA_pairs_econ_part2.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "client = openai.OpenAI()\n",
    "claude_client = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nonexcludable goods and services include police protection and most roads, as it is challenging to prevent individuals from benefiting from these once they are provided. Public primary education also falls into this category, as it is generally accessible to all, despite the existence of private options that may be excludable. In contrast, streaming music services, cell phone service, and toll roads are excludable, as access is limited to paying customers or users.',\n",
       " 'Among the options provided, roads and public primary education are considered nonexcludable. Most roads are nonexcludable because, once constructed, it is difficult to prevent individuals from using them, although toll roads can create exceptions. Public primary education is generally nonexcludable as it is available to all children regardless of payment. In contrast, police protection is excludable since access can be limited to those who pay for private security services. Similarly, streaming music services like SiriusXM are excludable due to their subscription-based model, and cell phone service is also excludable as companies can restrict access to paying customers.',\n",
       " 'Cell phone service is nonexcludable because it is available to everyone without any payment required. Similarly, police protection is also nonexcludable, as it is difficult to exclude individuals from benefiting from it once provided. Most roads are considered nonexcludable since, once built, it is challenging to prevent people from using them, although toll roads can create exceptions. Primary education is generally nonexcludable in the public sector, while private education can be excludable. In contrast, streaming music services like SiriusXM are typically subscription-based, making them excludable as access is restricted to paying customers.',\n",
       " 'Police protection is nonexcludable because it is challenging to prevent individuals from benefiting once it is provided. Most roads are excludable since access can be restricted through tolls or private ownership. Streaming music services, such as SiriusXM, are also excludable as they require a subscription for access. Primary education can be nonexcludable when provided publicly, but it can be excludable if offered by private entities. Cell phone service is excludable, as companies can limit access to paying customers.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['choices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers: 100%|██████████| 161/161 [04:41<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "log = defaultdict(dict)\n",
    "\n",
    "for generated_qa in tqdm(data, desc=\"Generating answers\"):\n",
    "    question = generated_qa['question']\n",
    "    correct_answer = generated_qa['correct_answer']\n",
    "    choices = generated_qa['mcq']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Question: {choices}\n",
    "    Which of the above choices best answers the question? Give a reason for your choice first, then give your choice.\n",
    "    Reply a JSON object with the following format:\n",
    "    {{\n",
    "        \"reason\": \"string\",\n",
    "        \"choice\": \"int\",\n",
    "    }}\n",
    "    \"\"\"\n",
    "    temperature = 0.0\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a student answering a textbook problem.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "    \n",
    "    answer_json = json.loads(response.choices[0].message.content)\n",
    "\n",
    "    log[question]['reason'] = answer_json['reason']\n",
    "    log[question]['gpt_choice'] = answer_json['choice']\n",
    "    log[question]['correct_choice'] = correct_answer + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct choices: 65.22%\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "total_count = len(log)\n",
    "\n",
    "for _, answers in log.items():\n",
    "    if answers['gpt_choice'] == answers['correct_choice'] + 1:\n",
    "        correct_count += 1\n",
    "\n",
    "percentage_correct = (correct_count / total_count) * 100\n",
    "print(f\"Percentage of correct choices: {percentage_correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reason': 'Choice D best answers the question because it correctly identifies police protection and most roads as nonexcludable, acknowledging the difficulty in preventing individuals from benefiting from these services once provided. It also correctly notes that public primary education is generally nonexcludable, while distinguishing that streaming music services, cell phone service, and toll roads are excludable due to their access restrictions.',\n",
       " 'gpt_choice': 4,\n",
       " 'correct_choice': 3}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log[list(log.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating answers:   0%|          | 0/161 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BetaMessageBatch(id='msgbatch_01C2C1yfU2Bac2nApYcucpAf', archived_at=None, cancel_initiated_at=None, created_at=datetime.datetime(2024, 11, 18, 0, 16, 50, 99607, tzinfo=datetime.timezone.utc), ended_at=None, expires_at=datetime.datetime(2024, 11, 19, 0, 16, 50, 99607, tzinfo=datetime.timezone.utc), processing_status='in_progress', request_counts=BetaMessageBatchRequestCounts(canceled=0, errored=0, expired=0, processing=1, succeeded=0), results_url=None, type='message_batch')\n"
     ]
    }
   ],
   "source": [
    "from anthropic.types.beta.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.beta.messages.batch_create_params import Request\n",
    "requests = []\n",
    "\n",
    "for generated_qa in tqdm(data, desc=\"Generating answers\"):\n",
    "    question = generated_qa['question']\n",
    "    correct_answer = generated_qa['correct_answer']\n",
    "    choices = generated_qa['mcq']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Question: {choices}\n",
    "    Which of the above choices best answers the question? Give a reason for your choice first, then give your choice as an integer between 0 and 3.\n",
    "    Reply only a JSON object with the following format:\n",
    "    {{\n",
    "        \"reason\": \"string\",\n",
    "        \"choice\": \"int\",\n",
    "    }}\n",
    "    \"\"\"\n",
    "    temperature = 0.0\n",
    "    params = MessageCreateParamsNonStreaming(\n",
    "                model=\"claude-3-5-haiku-latest\",\n",
    "                system = \"You are a student answering a textbook problem.\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=1000,\n",
    "            )\n",
    "    \n",
    "    custom_id = str(hash(question))\n",
    "    requests.append(Request(custom_id=custom_id, params=params))\n",
    "    break\n",
    "    \n",
    "message_batch = claude_client.beta.messages.batches.create(requests=requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnthropicError",
     "evalue": "No `results_url` for the given batch; Has it finished processing? in_progress",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnthropicError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m anthropic_log \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m      3\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m message_batch\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m claude_client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mresults(batch_id):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccess! \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mcustom_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\anthropic\\resources\\beta\\messages\\batches.py:324\u001b[0m, in \u001b[0;36mBatches.results\u001b[1;34m(self, message_batch_id, betas, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(message_batch_id\u001b[38;5;241m=\u001b[39mmessage_batch_id)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mresults_url:\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AnthropicError(\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo `results_url` for the given batch; Has it finished processing? \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m.\u001b[39mprocessing_status\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m     )\n\u001b[0;32m    328\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/binary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[0;32m    329\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstrip_not_given(\n\u001b[0;32m    331\u001b[0m         {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[0;32m    338\u001b[0m }\n",
      "\u001b[1;31mAnthropicError\u001b[0m: No `results_url` for the given batch; Has it finished processing? in_progress"
     ]
    }
   ],
   "source": [
    "anthropic_log = defaultdict(dict)\n",
    "\n",
    "batch_id = message_batch.id\n",
    "\n",
    "for result in claude_client.beta.messages.batches.results(batch_id):\n",
    "    if result.result.type == \"succeeded\":\n",
    "        print(f\"Success! {result.custom_id}\")\n",
    "\n",
    "\n",
    "# answer_json = json.loads(response.content[0].text)\n",
    "\n",
    "#     anthropic_log[question]['reason'] = answer_json['reason']\n",
    "#     anthropic_log[question]['gpt_choice'] = answer_json['choice']\n",
    "#     anthropic_log[question]['correct_choice'] = correct_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct choices: 10500.00%\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "total_count = len(anthropic_log)\n",
    "\n",
    "for _, answers in log.items():\n",
    "    if answers['gpt_choice'] == answers['correct_choice'] + 1:\n",
    "        correct_count += 1\n",
    "\n",
    "percentage_correct = (correct_count / total_count) * 100\n",
    "print(f\"Percentage of correct choices: {percentage_correct:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
