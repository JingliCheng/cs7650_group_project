{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shuffled_QA_pairs_econ.json') as f:\n",
    "    qa_pairs = json.load(f)\n",
    "\n",
    "\n",
    "with open('log_all.pkl', 'rb') as f:\n",
    "    result = pickle.load(f)\n",
    "\n",
    "with open('log_pick_wrong.pkl', 'rb') as f:\n",
    "    result_wrong = pickle.load(f)\n",
    "\n",
    "with open('log_pick_correct.pkl', 'rb') as f:\n",
    "    result_correct = pickle.load(f)\n",
    "\n",
    "with open('classification_log_all.pkl', 'rb') as f:\n",
    "    classification = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_set = set(result.keys())\n",
    "\n",
    "qa_result = []\n",
    "for q in question_set:\n",
    "    temp_dict = {\n",
    "        'question': q,\n",
    "        'gpt_choice': result[q]['gpt_choice'],\n",
    "        'incorrect_choices': result_wrong[q]['incorrect_choices'],\n",
    "        'correct_choices': result_correct[q]['correct_choices'],\n",
    "        'correct_choice': result[q]['correct_choice'],\n",
    "        'completeness_choice': classification[q]['completeness_choice'],\n",
    "        'type_choice': classification[q]['type_choice'],\n",
    "        'mcq_choice': classification[q]['mcq_choice'],\n",
    "        'completeness_reason': classification[q]['completeness_reason'],\n",
    "        'type_reason': classification[q]['type_reason'],\n",
    "        'mcq_reason': classification[q]['mcq_reason']\n",
    "    }\n",
    "    qa_result.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_result to dataframe\n",
    "df = pd.DataFrame(qa_result)\n",
    "\n",
    "completeness_name_map = {\n",
    "    1: 'Incomplete',\n",
    "    2: 'Missing reference',\n",
    "    3: 'Missing context',\n",
    "    9: 'Other',\n",
    "    0: 'Complete'\n",
    "}\n",
    "\n",
    "type_name_map = {\n",
    "    1: 'Conceptual',\n",
    "    2: 'Computation',\n",
    "    3: 'True/False',\n",
    "    4: 'Graphical',\n",
    "    5: 'Reasoning',\n",
    "    9: 'Other'\n",
    "}\n",
    "\n",
    "mcq_name_map = {\n",
    "    1: 'Fit for MCQ',\n",
    "    0: 'Not fit for MCQ'\n",
    "}\n",
    "\n",
    "df['completeness_name'] = df['completeness_choice'].map(completeness_name_map)\n",
    "df['type_name'] = df['type_choice'].map(type_name_map)\n",
    "df['mcq_name'] = df['mcq_choice'].map(mcq_name_map)\n",
    "\n",
    "df['reject'] = df.apply(lambda row: row['correct_choice'] not in row['incorrect_choices'], axis=1)\n",
    "df['within'] = df.apply(lambda row: row['correct_choice'] in row['correct_choices'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completeness\n",
    "- 1. Incomplete: Is the question complete? Are there any missing words or phrases, or references to other parts of the text that are not included?\n",
    "- 2. Missing reference: Does the question contain all the reference materials? Are there any missing tables, charts, or other information that is necessary to answer the question?\n",
    "- 3. Missing context: Does the question contain all the necessary context? Does it ask for information from sections in the textbook, which are not included in the question?\n",
    "- 9. Other: The question does not have enough information, but it does not fit into any of the above categories.\n",
    "- 0. Complete: The question has all the necessary information to be answered.\n",
    "\n",
    "Type\n",
    "- 1. Conceptual: The question asks for an explanation of a concept or theory.\n",
    "- 2. Computation: The question asks for a calculation or numerical answer.\n",
    "- 3. True/False: The question asks for a true or false answer.\n",
    "- 4. Graphical: The question asks for a graph or chart to be drawn.\n",
    "- 5. Reasoning: The question asks for a logical explanation or reasoning.\n",
    "- 9. Other: The question does not fit into any of the above categories.\n",
    "\n",
    "MCQ\n",
    "- 1. Yes: The question is fit for use as a multiple-choice question.\n",
    "- 0. No: The question is not fit for use as a multiple-choice question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>gpt_choice</th>\n",
       "      <th>incorrect_choices</th>\n",
       "      <th>correct_choices</th>\n",
       "      <th>correct_choice</th>\n",
       "      <th>completeness_choice</th>\n",
       "      <th>type_choice</th>\n",
       "      <th>mcq_choice</th>\n",
       "      <th>completeness_reason</th>\n",
       "      <th>type_reason</th>\n",
       "      <th>mcq_reason</th>\n",
       "      <th>completeness_name</th>\n",
       "      <th>type_name</th>\n",
       "      <th>mcq_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n6. \\n\\nDo the jobs for workers in low-income...</td>\n",
       "      <td>B</td>\n",
       "      <td>[1, 3, 4]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The question is complete as it provides all th...</td>\n",
       "      <td>The question is conceptual because it asks for...</td>\n",
       "      <td>This question is fit for use as a multiple-cho...</td>\n",
       "      <td>Complete</td>\n",
       "      <td>Conceptual</td>\n",
       "      <td>Fit for MCQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n12. \\n\\n  In a recession, does the actual bu...</td>\n",
       "      <td>2</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>The question is complete as it provides all th...</td>\n",
       "      <td>The question is conceptual as it asks for an e...</td>\n",
       "      <td>This question is fit for use as a multiple-cho...</td>\n",
       "      <td>Complete</td>\n",
       "      <td>Conceptual</td>\n",
       "      <td>Fit for MCQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n15. \\n\\nWhat is the difference between a fre...</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 2, 3]</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The question is complete as it provides all th...</td>\n",
       "      <td>The question is asking for an explanation of c...</td>\n",
       "      <td>This question is not fit for use as a multiple...</td>\n",
       "      <td>Complete</td>\n",
       "      <td>Conceptual</td>\n",
       "      <td>Not fit for MCQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n7. \\n\\n        List the areas where governme...</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2, 4]</td>\n",
       "      <td>[2, 3, 4]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The question is complete as it asks for a list...</td>\n",
       "      <td>The question is conceptual because it asks for...</td>\n",
       "      <td>This question is not fit for use as a multiple...</td>\n",
       "      <td>Complete</td>\n",
       "      <td>Conceptual</td>\n",
       "      <td>Not fit for MCQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n19. \\n\\nHow would a balanced budget amendmen...</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 3, 4]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The question is complete as it provides all th...</td>\n",
       "      <td>The question is conceptual because it asks for...</td>\n",
       "      <td>This question is not fit for use as a multiple...</td>\n",
       "      <td>Complete</td>\n",
       "      <td>Conceptual</td>\n",
       "      <td>Not fit for MCQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question gpt_choice  \\\n",
       "0  \\n6. \\n\\nDo the jobs for workers in low-income...          B   \n",
       "1  \\n12. \\n\\n  In a recession, does the actual bu...          2   \n",
       "2  \\n15. \\n\\nWhat is the difference between a fre...          4   \n",
       "3  \\n7. \\n\\n        List the areas where governme...          3   \n",
       "4  \\n19. \\n\\nHow would a balanced budget amendmen...          2   \n",
       "\n",
       "  incorrect_choices correct_choices  correct_choice  completeness_choice  \\\n",
       "0         [1, 3, 4]          [1, 2]               2                    0   \n",
       "1               [4]       [1, 2, 3]               2                    0   \n",
       "2         [1, 2, 3]          [1, 4]               4                    0   \n",
       "3         [1, 2, 4]       [2, 3, 4]               3                    0   \n",
       "4         [1, 3, 4]             [2]               2                    0   \n",
       "\n",
       "   type_choice  mcq_choice                                completeness_reason  \\\n",
       "0            1           1  The question is complete as it provides all th...   \n",
       "1            1           1  The question is complete as it provides all th...   \n",
       "2            1           0  The question is complete as it provides all th...   \n",
       "3            1           0  The question is complete as it asks for a list...   \n",
       "4            1           0  The question is complete as it provides all th...   \n",
       "\n",
       "                                         type_reason  \\\n",
       "0  The question is conceptual because it asks for...   \n",
       "1  The question is conceptual as it asks for an e...   \n",
       "2  The question is asking for an explanation of c...   \n",
       "3  The question is conceptual because it asks for...   \n",
       "4  The question is conceptual because it asks for...   \n",
       "\n",
       "                                          mcq_reason completeness_name  \\\n",
       "0  This question is fit for use as a multiple-cho...          Complete   \n",
       "1  This question is fit for use as a multiple-cho...          Complete   \n",
       "2  This question is not fit for use as a multiple...          Complete   \n",
       "3  This question is not fit for use as a multiple...          Complete   \n",
       "4  This question is not fit for use as a multiple...          Complete   \n",
       "\n",
       "    type_name         mcq_name  \n",
       "0  Conceptual      Fit for MCQ  \n",
       "1  Conceptual      Fit for MCQ  \n",
       "2  Conceptual  Not fit for MCQ  \n",
       "3  Conceptual  Not fit for MCQ  \n",
       "4  Conceptual  Not fit for MCQ  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Reject rate:\n",
    "reject means the gpt doesn't select the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.4970\n",
      "\n",
      "Reject rate: 0.8254\n",
      "\n",
      "within rate: 0.9438\n",
      "\n",
      "Distribution of completeness: completeness_name\n",
      "Complete             281\n",
      "Missing reference     47\n",
      "Missing context        7\n",
      "Incomplete             3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of type: type_name\n",
      "Conceptual     222\n",
      "Computation     53\n",
      "Reasoning       35\n",
      "Other           18\n",
      "Graphical        7\n",
      "True/False       3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution of mcq: mcq_name\n",
      "Not fit for MCQ    228\n",
      "Fit for MCQ        110\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy of gpt choice\n",
    "overall_accuracy = (df['gpt_choice'] == df['correct_choice']).mean()\n",
    "print(f\"Overall accuracy: {overall_accuracy:.4f}\\n\")\n",
    "\n",
    "# reject rate\n",
    "reject_rate = (df.apply(lambda row: row['correct_choice'] not in row['incorrect_choices'], axis=1)).mean()\n",
    "print(f\"Reject rate: {reject_rate:.4f}\\n\")\n",
    "\n",
    "# mean of correct choice in selected correct choices\n",
    "within_rate = (df.apply(lambda row: row['correct_choice'] in row['correct_choices'], axis=1)).mean()\n",
    "print(f\"within rate: {within_rate:.4f}\\n\")\n",
    "\n",
    "# distribution of completeness choice\n",
    "completeness_dist = df['completeness_name'].value_counts()\n",
    "print(f\"Distribution of completeness: {completeness_dist}\\n\")\n",
    "\n",
    "# distribution of type choice\n",
    "type_dist = df['type_name'].value_counts()\n",
    "print(f\"Distribution of type: {type_dist}\\n\")\n",
    "\n",
    "# distribution of mcq choice\n",
    "mcq_dist = df['mcq_name'].value_counts()\n",
    "print(f\"Distribution of mcq: {mcq_dist}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by completeness: completeness_name\n",
      "Complete             0.505338\n",
      "Incomplete           0.333333\n",
      "Missing context      0.571429\n",
      "Missing reference    0.446809\n",
      "dtype: float64\n",
      "\n",
      "Reject rate by completeness: completeness_name\n",
      "Complete             0.832740\n",
      "Incomplete           0.666667\n",
      "Missing context      0.857143\n",
      "Missing reference    0.787234\n",
      "dtype: float64\n",
      "\n",
      "Within rate by completeness: completeness_name\n",
      "Complete             0.946619\n",
      "Incomplete           0.666667\n",
      "Missing context      1.000000\n",
      "Missing reference    0.936170\n",
      "dtype: float64\n",
      "\n",
      "========================================\n",
      "Accuracy by type: type_name\n",
      "Computation    0.358491\n",
      "Conceptual     0.563063\n",
      "Graphical      0.000000\n",
      "Other          0.333333\n",
      "Reasoning      0.514286\n",
      "True/False     0.000000\n",
      "dtype: float64\n",
      "\n",
      "Reject rate by type: type_name\n",
      "Computation    0.679245\n",
      "Conceptual     0.873874\n",
      "Graphical      0.571429\n",
      "Other          0.666667\n",
      "Reasoning      0.885714\n",
      "True/False     0.666667\n",
      "dtype: float64\n",
      "\n",
      "Within rate by type: type_name\n",
      "Computation    0.867925\n",
      "Conceptual     0.959459\n",
      "Graphical      1.000000\n",
      "Other          0.888889\n",
      "Reasoning      1.000000\n",
      "True/False     0.666667\n",
      "dtype: float64\n",
      "\n",
      "========================================\n",
      "Accuracy by mcq: mcq_name\n",
      "Fit for MCQ        0.527273\n",
      "Not fit for MCQ    0.482456\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy by completeness\n",
    "completeness_accuracy = df.groupby('completeness_name')[['gpt_choice', 'correct_choice']].apply(lambda x: (x['gpt_choice'] == x['correct_choice']).mean())\n",
    "print(f\"Accuracy by completeness: {completeness_accuracy}\\n\")\n",
    "\n",
    "# reject rate by completeness\n",
    "completeness_reject_rate = df.groupby('completeness_name')[['reject']].apply(lambda x: x['reject'].mean())\n",
    "print(f\"Reject rate by completeness: {completeness_reject_rate}\\n\")\n",
    "\n",
    "# within rate by completeness\n",
    "completeness_within_rate = df.groupby('completeness_name')[['within']].apply(lambda x: x['within'].mean())\n",
    "print(f\"Within rate by completeness: {completeness_within_rate}\\n\")\n",
    "\n",
    "print('=='*20)\n",
    "# accuracy by type\n",
    "type_accuracy = df.groupby('type_name')[['gpt_choice', 'correct_choice']].apply(lambda x: (x['gpt_choice'] == x['correct_choice']).mean())\n",
    "print(f\"Accuracy by type: {type_accuracy}\\n\")\n",
    "\n",
    "# reject rate by type\n",
    "type_reject_rate = df.groupby('type_name')[['reject']].apply(lambda x: x['reject'].mean())\n",
    "print(f\"Reject rate by type: {type_reject_rate}\\n\")\n",
    "\n",
    "# within rate by type\n",
    "type_within_rate = df.groupby('type_name')[['within']].apply(lambda x: x['within'].mean())\n",
    "print(f\"Within rate by type: {type_within_rate}\\n\")\n",
    "\n",
    "print('=='*20)\n",
    "# accuracy by mcq\n",
    "mcq_accuracy = df.groupby('mcq_name')[['gpt_choice', 'correct_choice']].apply(lambda x: (x['gpt_choice'] == x['correct_choice']).mean())\n",
    "print(f\"Accuracy by mcq: {mcq_accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of our filter\n",
    "\n",
    "- filter 1: completeness in [0] and type in [1, 2, 5]\n",
    "- filter 2: completeness in [0] and type in [1, 2, 5] and mcq in [1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filter_1'] = (df['completeness_choice'] == 0) & (df['type_choice'].isin([1, 2, 5]))\n",
    "df['filter_2'] = (df['completeness_choice'] == 0) & (df['type_choice'].isin([1, 2, 5])) & (df['mcq_choice'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by filter 1: filter_1\n",
      "False    0.373333\n",
      "True     0.532319\n",
      "dtype: float64\n",
      "\n",
      "Reject rate by filter 1: filter_1\n",
      "False    0.760000\n",
      "True     0.844106\n",
      "dtype: float64\n",
      "\n",
      "Within rate by filter 1: filter_1\n",
      "False    0.92000\n",
      "True     0.95057\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy by fitler\n",
    "filter_1_accuracy = df.groupby('filter_1')[['gpt_choice', 'correct_choice']].apply(lambda x: (x['gpt_choice'] == x['correct_choice']).mean())\n",
    "filter_1_reject_rate = df.groupby('filter_1')[['reject']].apply(lambda x: x['reject'].mean())\n",
    "filter_1_within_rate = df.groupby('filter_1')[['within']].apply(lambda x: x['within'].mean())\n",
    "\n",
    "\n",
    "print(f\"Accuracy by filter 1: {filter_1_accuracy}\\n\")\n",
    "print(f\"Reject rate by filter 1: {filter_1_reject_rate}\\n\")\n",
    "print(f\"Within rate by filter 1: {filter_1_within_rate}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FSIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
